{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xai import *\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 8\n",
    "\n",
    "try:\n",
    "    mnist = MNIST.load(\"mnist.model\")\n",
    "except FileNotFoundError:\n",
    "    mnist = MNIST(\n",
    "        latent_shape=(latent_dim,), \n",
    "        hidden_layers=2, \n",
    "        classifier_head_output_activation=\"Softmax\",\n",
    "        ) \n",
    "    \n",
    "    mnist.fit_autoencoder(\n",
    "        epochs=10_000,\n",
    "        batch_size=256,\n",
    "        loss_criterion=\"MSELoss\",\n",
    "        early_stop_cont=750,\n",
    "        verbose=True,\n",
    "        info=\"Mnist autoencoder train\"\n",
    "    )\n",
    "\n",
    "    mnist.fit_classifier_head(\n",
    "        epochs=10_000,\n",
    "        batch_size=256,\n",
    "        early_stop_cont=750,\n",
    "        verbose=True,\n",
    "        info=\"Mnist classifier-head train\"\n",
    "    )\n",
    "\n",
    "    mnist.save(\"mnist.model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.autoencoder.train_history.figure(\"Mnist autoencoder train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.classifier_head.train_history.figure(\"Mnist classifier-head train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"./images/mnist-feature-space-explanation\"\n",
    "try:\n",
    "    os.mkdir(save_folder)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for algorithm in [\"exact\", \"permutation\", \"deep\", \"kernel\", \"gradient\"]:\n",
    "    try:\n",
    "        os.mkdir(f\"{save_folder}/{algorithm}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "rows = 10\n",
    "try:\n",
    "    samples = Stream.load(\"images/samples\", Tuple[torch.Tensor,torch.Tensor]).list()\n",
    "except:\n",
    "    samples: List[torch.Tensor,torch.Tensor] = []\n",
    "    for i in range(rows):\n",
    "        samples.append(mnist.get_sample(digit=i))\n",
    "\n",
    "    Stream(samples).save(\"images/samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_shap_latent_plot(algorithm: Explainers):\n",
    "    fig = plt.figure(dpi=100, figsize=(20,28)) \n",
    "    fig.suptitle(f\"Algorithm: {algorithm.capitalize()}Explainer\", size=30)\n",
    "    columns = latent_dim + 3\n",
    "\n",
    "    def save_mnist(image: np.ndarray, path: str, **args) -> None:\n",
    "        plt.imsave(path, image.repeat(16,0).repeat(16,1), **args)\n",
    "\n",
    "    compute_times = []\n",
    "\n",
    "    with tqdm(total=rows, desc=f\"{algorithm.capitalize()}Explainer\") as bar:\n",
    "        for row in range(rows):\n",
    "            slot = row*columns\n",
    "\n",
    "            sample,label = samples[row]\n",
    "            sample = sample.numpy(force=True)\n",
    "            label = int(label.item())\n",
    "            predict = mnist(sample)\n",
    "            recon = predict.reconstruction().numpy(force=True)\n",
    "\n",
    "            latent_explanation = predict.reconstruction.explain(algorithm, mnist(mnist.val_data).embedding()).invert()\n",
    "            shap_values: np.ndarray = latent_explanation.shap_values\n",
    "            norm = max([np.max(np.abs(shap_values))])\n",
    "\n",
    "            compute_times.append(latent_explanation.compute_time.float())\n",
    "\n",
    "            fig.add_subplot(rows,columns,slot+1)\n",
    "            plt.imshow(sample, cmap=\"gray\")\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"$x={{{label}}}$\", size=25)\n",
    "            save_mnist(sample, f\"{save_folder}/{algorithm}/sample{row}.png\", cmap=\"gray\")\n",
    "                \n",
    "            fig.add_subplot(rows,columns,slot+2)\n",
    "            plt.imshow(recon, cmap=\"gray\")\n",
    "            plt.axis('off')\n",
    "            plt.title(\"$\\hat{{x}}$\", size=25)\n",
    "            save_mnist(recon, f\"{save_folder}/{algorithm}/reconstruction{row}.png\", cmap=\"gray\")\n",
    "\n",
    "            for j,shap in enumerate(shap_values):\n",
    "                fig.add_subplot(rows,columns,slot+j+3)\n",
    "                im = np.zeros((28,28,3), dtype=np.float32)\n",
    "                red = np.where(shap > 0, shap/norm, np.zeros_like(shap))\n",
    "                blue = np.where(shap < 0, -shap/norm, np.zeros_like(shap))\n",
    "                im[:,:,0] = red\n",
    "                im[:,:,2] = blue\n",
    "                plt.imshow(im)\n",
    "                plt.axis('off')\n",
    "                plt.title(f\"$l_{j}$\", size=25)\n",
    "                save_mnist(im, f\"{save_folder}/{algorithm}/latent{j}-explanation.png\")\n",
    "            \n",
    "            bar.update()\n",
    "\n",
    "            shap_sum = shap_values.sum(0)\n",
    "            norm = np.max(np.abs(shap_sum))\n",
    "            im = np.zeros((28,28,3), dtype=np.float32)\n",
    "            red = np.where(shap_sum > 0, shap_sum/norm, np.zeros_like(shap_sum))\n",
    "            blue = np.where(shap_sum < 0, -shap_sum/norm, np.zeros_like(shap_sum))\n",
    "            im[:,:,0] = red\n",
    "            im[:,:,2] = blue\n",
    "            fig.add_subplot(rows,columns,slot+j+4)\n",
    "            plt.imshow(im)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"$\\sum_{{i=0}}^{latent_dim}l_i$\", size=25)\n",
    "            save_mnist(im, f\"{save_folder}/{algorithm}/latents-sum{row}.png\")\n",
    "\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{save_folder}/{algorithm}/shap-latents.png\", bbox_inches=\"tight\")\n",
    "\n",
    "    plt.close()\n",
    "    plt.plot(range(len(compute_times)), compute_times, label=\"Time used\")\n",
    "    plt.plot(range(len(compute_times)), [sum(compute_times)/len(compute_times)]*len(compute_times), label=\"Average time\")\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Seconds\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.title(f\"Compute times: {algorithm}\")\n",
    "    plt.savefig(f\"{save_folder}/{algorithm}/compute-times.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "algorithm: Explainers\n",
    "for algorithm in [\"exact\", \"permutation\", \"deep\", \"kernel\", \"gradient\"]:\n",
    "    mk_shap_latent_plot(algorithm)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
