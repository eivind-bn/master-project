{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xai import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "try:\n",
    "    dqn = DQN.load(\"dqn-model.pt\", device=device)\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating new agent...\")\n",
    "    dqn = DQN(autoencoder_path=\"asteroids-autoencoder-l32.pt\", translate=True, rotate=True, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.train(\n",
    "    total_time_steps=1_000_000,\n",
    "    replay_buffer_size=int(5e6),\n",
    "    learning_rate = 1e-4,\n",
    "    learning_starts = 6500,\n",
    "    batch_size = 64,\n",
    "    tau = 1.0,\n",
    "    gamma = 0.99,\n",
    "    train_frequency = 64,\n",
    "    frame_skip=4,\n",
    "    gradient_steps = 1,\n",
    "    episode_save_freq= 10,\n",
    "    target_update_frequency = 2000,\n",
    "    final_exploration_rate_progress = 0.3,\n",
    "    initial_exploration_rate = 1.0,\n",
    "    final_exploration_rate = 0.3,\n",
    "    verbose = True,\n",
    "    save_path=\"dqn-model.pt\",\n",
    "    q_value_head_background_path=\"states.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "WindowClosed",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWindowClosed\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Window(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsteroids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m4.0\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m window:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m dqn\u001b[38;5;241m.\u001b[39mrollout(\u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m50000\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mwindow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-project/xai/window.py:28\u001b[0m, in \u001b[0;36mWindowInterface.__call__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: NDArray[uint8]\u001b[38;5;241m|\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Events:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-project/xai/window.py:22\u001b[0m, in \u001b[0;36mWindowInterface.update\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: NDArray[uint8]\u001b[38;5;241m|\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Events:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_updater\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-project/xai/window.py:86\u001b[0m, in \u001b[0;36mWindow._update_and_poll_events\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     83\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_end\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_window_visible():\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WindowClosed()\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, image[:,:,::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flip_color_endianness \u001b[38;5;28;01melse\u001b[39;00m image)\n",
      "\u001b[0;31mWindowClosed\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with Window(\"Asteroids\", 60, 4.0) as window:\n",
    "    for step in dqn.rollout(0.7, 4).take(50000):\n",
    "        window(step.observation.numpy(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(dqn.rewards_per_episode)), dqn.rewards_per_episode, label=\"Total reward\")\n",
    "plt.plot(range(len(dqn.exploration_rate_per_episode)), dqn.exploration_rate_per_episode, label=\"Exploration rate\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(dqn.rewards_per_episode)), dqn.rewards_per_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Frame::   0%|          | 8/3000 [00:21<1:58:04,  2.37s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from xai import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import cv2\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "try:\n",
    "    dqn = DQN.load(\"dqn-model.pt\", device=device)\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating new agent...\")\n",
    "    dqn = DQN(autoencoder_path=\"asteroids-autoencoder-l32.pt\", translate=True, rotate=True, device=device) \n",
    "\n",
    "org = (10,4*207)\n",
    "fontscale = 4\n",
    "thickness = 3\n",
    "\n",
    "with Recorder(\"agent.mp4\", scale=4, fps=24) as recorder:\n",
    "    with Window(\"Asteroids\", 24, 4.0) as window:\n",
    "        obs_background = dqn._autoencoder.encoder(torch.load(\"observations.pt\")).output()\n",
    "        state_background = torch.from_numpy(np.load(\"states.npy\"))\n",
    "\n",
    "        for step in dqn.rollout(0.6, frame_skips=4).take(3000).monitor(\"Frame:\", expected_length=3000):\n",
    "            gc.collect()\n",
    "            original = step.observation.numpy(False).repeat(4,0).repeat(4,1)\n",
    "            transformed = step.observation.translated().rotated().numpy(False)\n",
    "            reconstruction = dqn._autoencoder(transformed.astype(float)/255.0).output().numpy(force=True)\n",
    "            transformed = transformed.repeat(4,0).repeat(4,1)\n",
    "            reconstruction = (reconstruction*255).astype(np.uint8).repeat(4,0).repeat(4,1)\n",
    "            q_values = dqn._policy(step.next_state).output().numpy(force=True)\n",
    "\n",
    "            original = cv2.putText(\n",
    "                img=original, \n",
    "                text=f\"Original\", \n",
    "                org=org , \n",
    "                fontFace=1,  \n",
    "                fontScale=fontscale, \n",
    "                color=(255,255,255), \n",
    "                thickness=thickness, \n",
    "                lineType=cv2.LINE_AA\n",
    "                ) \n",
    "            \n",
    "            transformed = cv2.putText(\n",
    "                img=transformed, \n",
    "                text=f\"Observation\", \n",
    "                org=org , \n",
    "                fontFace=1,  \n",
    "                fontScale=fontscale, \n",
    "                color=(255,255,255), \n",
    "                thickness=thickness, \n",
    "                lineType=cv2.LINE_AA\n",
    "                ) \n",
    "            \n",
    "            reconstruction = cv2.putText(\n",
    "                img=reconstruction, \n",
    "                text=f\"Reconstruction\", \n",
    "                org=org , \n",
    "                fontFace=1,  \n",
    "                fontScale=fontscale, \n",
    "                color=(255,255,255), \n",
    "                thickness=thickness, \n",
    "                lineType=cv2.LINE_AA\n",
    "                ) \n",
    "\n",
    "            explanations = step.explain(\n",
    "                algorithm=\"permutation\",\n",
    "                decoder_background=obs_background[:5],\n",
    "                q_background=state_background[:5]\n",
    "            )\n",
    "\n",
    "            eap_shap_values = explanations.eap_explanation.shap_values\n",
    "\n",
    "            eap_shap_values = np.append(eap_shap_values, eap_shap_values.sum(0, keepdims=True), axis=0)\n",
    "            norm = np.max(np.abs(eap_shap_values))\n",
    "\n",
    "            eap_images = np.zeros((len(eap_shap_values),210,160,3), dtype=np.uint8)\n",
    "            black = np.zeros_like(eap_shap_values)\n",
    "            red = np.where(eap_shap_values > 0, eap_shap_values/norm, black)\n",
    "            blue = np.where(eap_shap_values < 0, -eap_shap_values/norm, black)\n",
    "            eap_images[:,:,:,0] = (red*255).astype(np.uint8)\n",
    "            eap_images[:,:,:,2] = (blue*255).astype(np.uint8)\n",
    "\n",
    "            actions = (\n",
    "                \"NOOP\",\n",
    "                \"UP\",\n",
    "                \"LEFT\",\n",
    "                \"RIGHT\",\n",
    "                \"FIRE\"\n",
    "                )\n",
    "\n",
    "            eap_images = eap_images.repeat(4,1).repeat(4,2)\n",
    "\n",
    "            for i,eap_image in enumerate(eap_images[:-1]):\n",
    "                eap_images[i] = cv2.putText(\n",
    "                    img=eap_images[i], \n",
    "                    text=f\"Q-{actions[i]}: {q_values[i]:.2f}\", \n",
    "                    org=org , \n",
    "                    fontFace=1,  \n",
    "                    fontScale=fontscale, \n",
    "                    color=(255,255,255), \n",
    "                    thickness=thickness, \n",
    "                    lineType=cv2.LINE_AA\n",
    "                    ) \n",
    "                \n",
    "                \n",
    "            eap_images[-1] = cv2.putText(\n",
    "                img=eap_images[-1], \n",
    "                text=f\"Q-sum\", \n",
    "                org=org , \n",
    "                fontFace=1,  \n",
    "                fontScale=fontscale, \n",
    "                color=(255,255,255), \n",
    "                thickness=thickness, \n",
    "                lineType=cv2.LINE_AA\n",
    "                ) \n",
    "\n",
    "            ecp_shap_values = explanations.ecp_explanation.shap_values\n",
    "\n",
    "            ecp_shap_values = np.append(ecp_shap_values, ecp_shap_values.sum(0, keepdims=True), axis=0)\n",
    "            norm = np.max(np.abs(ecp_shap_values))\n",
    "\n",
    "            ecp_images = np.zeros((len(ecp_shap_values),210,160,3), dtype=np.uint8)\n",
    "\n",
    "            red = np.where(ecp_shap_values > 0, ecp_shap_values/norm, black)\n",
    "            blue = np.where(ecp_shap_values < 0, -ecp_shap_values/norm, black)\n",
    "            ecp_images[:,:,:,0] = (red*255).astype(np.uint8)\n",
    "            ecp_images[:,:,:,2] = (blue*255).astype(np.uint8)\n",
    "\n",
    "            ecp_images = ecp_images.repeat(4,1).repeat(4,2)\n",
    "\n",
    "            for i,ecp_image in enumerate(ecp_images[:-1]):\n",
    "                ecp_images[i] = cv2.putText(\n",
    "                    img=ecp_images[i], \n",
    "                    text=f\"Q-{actions[i]}: {q_values[i]:.2f}\", \n",
    "                    org=org , \n",
    "                    fontFace=1,  \n",
    "                    fontScale=fontscale, \n",
    "                    color=(255,255,255), \n",
    "                    thickness=thickness, \n",
    "                    lineType=cv2.LINE_AA\n",
    "                    ) \n",
    "                \n",
    "            ecp_images[-1] = cv2.putText(\n",
    "                img=ecp_images[-1], \n",
    "                text=f\"Q-sum\", \n",
    "                org=org , \n",
    "                fontFace=1,  \n",
    "                fontScale=fontscale, \n",
    "                color=(255,255,255), \n",
    "                thickness=thickness, \n",
    "                lineType=cv2.LINE_AA\n",
    "                ) \n",
    "\n",
    "\n",
    "            l_shap_values = explanations.latent_explanation.flip().shap_values.sum((0,3))\n",
    "            norm = np.max(np.abs(l_shap_values))\n",
    "\n",
    "            l_image = np.zeros((210,160,3), dtype=np.uint8)\n",
    "            black = np.zeros_like(l_shap_values)\n",
    "            red = np.where(l_shap_values > 0, l_shap_values/norm, black)\n",
    "            blue = np.where(l_shap_values < 0, -l_shap_values/norm, black)\n",
    "            l_image[:,:,0] = (red*255).astype(np.uint8)\n",
    "            l_image[:,:,2] = (blue*255).astype(np.uint8)\n",
    "\n",
    "            l_image = l_image.repeat(4,0).repeat(4,1)\n",
    "\n",
    "            l_image = cv2.putText(\n",
    "                img=l_image, \n",
    "                text=f\"latent-SHAP\", \n",
    "                org=org , \n",
    "                fontFace=1,  \n",
    "                fontScale=fontscale, \n",
    "                color=(255,255,255), \n",
    "                thickness=thickness, \n",
    "                lineType=cv2.LINE_AA\n",
    "                ) \n",
    "\n",
    "            black_image = np.zeros_like(original)\n",
    "\n",
    "            image = np.hstack([\n",
    "                np.vstack([original,transformed,reconstruction]),\n",
    "                np.vstack([black_image,l_image,black_image]),\n",
    "                np.vstack(eap_images[:3]),\n",
    "                np.vstack(eap_images[3:]),\n",
    "                np.vstack(ecp_images[:3]),\n",
    "                np.vstack(ecp_images[3:]),\n",
    "                ])\n",
    "            window(image)\n",
    "            recorder(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = {}\n",
    "\n",
    "obs_background = dqn._autoencoder.encoder(torch.load(\"observations.pt\")).output()\n",
    "state_background = torch.from_numpy(np.load(\"states.npy\"))\n",
    "\n",
    "for step in dqn.rollout(0.7, frame_skips=4).take(240).monitor(\"Frame:\", expected_length=3000):\n",
    "    obs = step.observation.numpy(True)\n",
    "    videos.setdefault(\"Original\", []).append(step.observation.numpy(False))\n",
    "    videos.setdefault(\"Affine\", []).append(step.observation.translated().rotated().numpy(False))\n",
    "\n",
    "    eap_shap_values = step.explain_eap(\n",
    "        \"permutation\",\n",
    "        decoder_background=obs_background[:50],\n",
    "        q_background=state_background[:50]\n",
    "    ).shap_values\n",
    "    shap_sum = eap_shap_values.sum(0)\n",
    "\n",
    "    norm = max([np.max(np.abs(eap_shap_values)), np.max(np.abs(shap_sum))])\n",
    "\n",
    "    for i,action_explanation in enumerate(eap_shap_values):\n",
    "        eap_images = np.zeros((210,160,3), dtype=np.uint8)\n",
    "        black = np.zeros_like(action_explanation)\n",
    "        red = np.where(action_explanation > 0, action_explanation/norm, black)\n",
    "        blue = np.where(action_explanation < 0, -action_explanation/norm, black)\n",
    "        eap_images[:,:,0] = (red*255).astype(np.uint8)\n",
    "        eap_images[:,:,2] = (blue*255).astype(np.uint8)\n",
    "        videos.setdefault(i, []).append(eap_images)\n",
    "\n",
    "    eap_images = np.zeros((210,160,3), dtype=np.uint8)\n",
    "    black = np.zeros_like(shap_sum)\n",
    "    red = np.where(shap_sum > 0, shap_sum/norm, black)\n",
    "    blue = np.where(shap_sum < 0, -shap_sum/norm, black)\n",
    "    eap_images[:,:,0] = (red*255).astype(np.uint8)\n",
    "    eap_images[:,:,2] = (blue*255).astype(np.uint8)\n",
    "    videos.setdefault(\"Sum\", []).append(eap_images)\n",
    "\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_action = {\n",
    "    0: \"Noop.mp4\",\n",
    "    1: \"Up.mp4\",\n",
    "    2: \"Left.mp4\",\n",
    "    3: \"Right.mp4\",\n",
    "    4: \"Fire.mp4\"\n",
    "}\n",
    "\n",
    "for i,name in id_to_action.items():\n",
    "    with Recorder(f\"Videos/DQN-EAP-SHAP/{name}\", fps=24, scale=4) as recorder:\n",
    "        for frame in videos[i]:\n",
    "            recorder(frame)\n",
    "\n",
    "with Recorder(\"Videos/DQN-EAP-SHAP/Original.mp4\", fps=24, scale=4) as recorder:\n",
    "    for frame in videos[\"Original\"]:\n",
    "        recorder(frame)\n",
    "\n",
    "with Recorder(\"Videos/DQN-EAP-SHAP/Affine.mp4\", fps=24, scale=4) as recorder:\n",
    "    for frame in videos[\"Affine\"]:\n",
    "        recorder(frame)\n",
    "\n",
    "with Recorder(\"Videos/DQN-EAP-SHAP/Sum.mp4\", fps=24, scale=4) as recorder:\n",
    "    for frame in videos[\"Sum\"]:\n",
    "        recorder(frame)\n",
    "\n",
    "with Recorder(\"Videos/DQN-EAP-SHAP/Combined.mp4\", fps=24, scale=3) as recorder:\n",
    "    for frames in np.moveaxis(np.array(list(videos.values())),1,0):\n",
    "        recorder(np.hstack(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Window(\"Asteroids\", 60, 4.0) as window:\n",
    "    for i,step in dqn.rollout(0.7, frame_skips=4).take(3000).enumerate():\n",
    "        window(step.observation.numpy(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
